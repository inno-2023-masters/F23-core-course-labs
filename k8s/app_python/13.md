We created a stateful set from scratch, which later on turned out to be almost identical to the prior deployment...
``````
$ kubectl --namespace=app_python get po,sts,svc,pvc,pv
NAME               READY   STATUS    RESTARTS   AGE
pod/app_python-0   1/1     Running   0          23m
pod/app_python-1   1/1     Running   0          22m
pod/app_python-2   1/1     Running   0          22m

NAME                          READY   AGE
statefulset.apps/app_python   3/3     23m

NAME                 TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
service/app_python   LoadBalancer   10.98.142.76   <pending>     8000:30000/TCP   23m

NAME                                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/visits-app_python-0   Bound    pvc-248c9673-1a52-436d-a7fb-a757f54532b1   32Mi       RWX            standard       23m
persistentvolumeclaim/visits-app_python-1   Bound    pvc-940929bb-49f4-4f87-9ae7-2899a5d7cc10   32Mi       RWX            standard       22m
persistentvolumeclaim/visits-app_python-2   Bound    pvc-fc36fab8-9e40-426e-a42c-7012eca036c0   32Mi       RWX            standard       22m

NAME                                                        CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                            STORAGECLASS   REASON   AGE
persistentvolume/pvc-248c9673-1a52-436d-a7fb-a757f54532b1   32Mi       RWX            Delete           Bound    app_python/visits-app_python-0   standard                22m
persistentvolume/pvc-940929bb-49f4-4f87-9ae7-2899a5d7cc10   32Mi       RWX            Delete           Bound    app_python/visits-app_python-1   standard                22m
persistentvolume/pvc-fc36fab8-9e40-426e-a42c-7012eca036c0   32Mi       RWX            Delete           Bound    app_python/visits-app_python-2   standard                22m

``````

We can access the service with
``````
$ minikube --namespace=app_python service app_python
``````

## Ordering Guarantee and Parallel Operations
``````
The necessity of ordering guarantees depends on the application's specific requirements. If the application is stateless and can function independently of pod order, or if scalability and load distribution are crucial, ordering guarantees may be unnecessary.
``````

After refreshing the page several times, we can view the following values in the visits log
```````
$ kubectl --namespace=app_python exec pod/app_python-0 -- cat /var/app_python/visits
17:36:47.511070
17:36:48.645548
18:24:43.809137
18:24:43.956889
18:24:44.545806
18:24:44.697377
18:24:44.970476
$ kubectl --namespace=app_python exec pod/app_python-1 -- cat /var/app_python/visits
17:36:48.019629
17:36:53.701679
17:37:17.575620
18:04:19.218328
18:24:44.092608
18:24:44.243052
18:24:45.099598
$ kubectl --namespace=app_python exec pod/app_python-2 -- cat /var/app_python/visits
17:36:48.164605
17:36:48.333706
17:36:48.502382
17:36:48.800126
17:36:48.948866
18:24:43.663598
18:24:44.392815

``````
Refreshing this page gives different results since the load balancer forwards to different pods, which in turn fetch data from different persistent volumes. We can verify this with
``````
$ kubectl --namespace=app_python get pv
NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                            STORAGECLASS   REASON   AGE
pvc-248c9673-1a52-436d-a7fb-a757f54532b1   32Mi       RWX            Delete           Bound    app_python/visits-app_python-0   standard                50m
pvc-940929bb-49f4-4f87-9ae7-2899a5d7cc10   32Mi       RWX            Delete           Bound    app_python/visits-app_python-1   standard                50m
pvc-fc36fab8-9e40-426e-a42c-7012eca036c0   32Mi       RWX            Delete           Bound    app_python/visits-app_python-2   standard                49m

``````
Indeed, there are three different PV's assigned to three different PVC's. Without any syncing mechanism, these PV's will naturally store different data.

In our app we do not need ordered pod launch, since our pods don't have any dependency among each other. We can inform the cluster and specify spec.pod Management Policy: Parallel in the stateful set manifest.