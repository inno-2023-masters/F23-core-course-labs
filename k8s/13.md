# StatefulSet

Azamat Shakirov B20-CS

a.shakirov@innopolis.university





---

### StatefulSet output:

Output of `kubectl get po,sts,svc,pvc`:

```bash
NAME                                        READY   STATUS    RESTARTS        AGE
pod/app-python-0                            1/1     Running   0               101s
pod/app-python-1                            1/1     Running   0               81s
pod/vault-0                                 1/1     Running   3 (19h ago)     13d
pod/vault-agent-injector-5cd8b87c6c-rhcdm   1/1     Running   3 (6d13h ago)   13d

NAME                          READY   AGE
statefulset.apps/app-python   2/2     101s
statefulset.apps/vault        1/1     13d

NAME                               TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)             AGE
service/app-python                 LoadBalancer   10.97.12.194     <pending>     5000:30398/TCP      101s
service/kubernetes                 ClusterIP      10.96.0.1        <none>        443/TCP             28d
service/vault                      ClusterIP      10.103.249.137   <none>        8200/TCP,8201/TCP   13d
service/vault-agent-injector-svc   ClusterIP      10.100.117.54    <none>        443/TCP             13d
service/vault-internal             ClusterIP      None             <none>        8200/TCP,8201/TCP   13d

NAME                                         STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/counter-app-python-0   Bound    pvc-4ea43ed5-947c-4dfd-941d-f32022879b03   10Mi       RWO            standard       19h
persistentvolumeclaim/counter-app-python-1   Bound    pvc-2a8cfc07-2563-4d6e-96b1-9e67e190d8d5   10Mi       RWO            standard       16h
```

Output of visits files:

```bash
kubectl exec pod/app-python-0 -- cat /assets/visits
48

kubectl exec pod/app-python-1 -- cat /assets/visits
50
```

**The result is different, because each pod has its own storage, and we visit service by different pods**

Ordering guarantees are unnecessary because in our application there is no dependency in data between pods

To launch all pods in parallel we can use `podManagementPolicy: Parallel` option (configured in **values.yaml** and in statefulset.yaml **spec.podManagementPolicy**)