# Lab 13: Kubernetes StatefulSet


```bash
$ kubectl get po,sts,svc,pvc
NAME                                        READY   STATUS    RESTARTS      AGE
pod/app-python-chart-0                      2/2     Running   0             78s
pod/app-python-chart-1                      2/2     Running   0             78s
pod/vault-0                                 1/1     Running   2 (65m ago)   13d
pod/vault-agent-injector-5cd8b87c6c-fhxvs   1/1     Running   2 (65m ago)   13d

NAME                                READY   AGE
statefulset.apps/app-python-chart   2/2     78s
statefulset.apps/vault              1/1     13d

NAME                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
service/app-python-chart           NodePort    10.107.129.32   <none>        80:31137/TCP        78s
service/kubernetes                 ClusterIP   10.96.0.1       <none>        443/TCP             28d
service/vault                      ClusterIP   10.107.161.46   <none>        8200/TCP,8201/TCP   13d
service/vault-agent-injector-svc   ClusterIP   10.109.31.149   <none>        443/TCP             13d
service/vault-internal             ClusterIP   None            <none>        8200/TCP,8201/TCP   13d

NAME                                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/appdata-py-app-python-chart-0   Bound    pvc-d1b41986-e827-415e-b20b-90a19bb8cb4c   15Mi       RWO            standard       78s
persistentvolumeclaim/appdata-py-app-python-chart-1   Bound    pvc-41f4c94b-7e79-4dcd-9c40-4334e987c2ab   15Mi       RWO            standard       78s
```

```bash
$ kubectl exec pod/app-python-chart-0 -- cat /appdata/visits.txt
396
```

```bash
$ kubectl exec pod/app-python-chart-1 -- cat /appdata/visits.txt
359
```

The difference in visits counts is due to load balancer distributing the requests between replicas. Having different files with different contents in the volume is possible, since each pod has its own persistent volume.


### Why ordering guarantees not necessary?
Since we are running two replicas of the same app without any mutual dependencies, we don't need any ordering guarantees.

### Implement a way to instruct the StatefulSet controller to launch or terminate all Pods in parallel.

We can achieve this via settings `.spec.podManagementPolicy` to `Parallel`:
> `Parallel` pod management tells the StatefulSet controller to launch or terminate all Pods in parallel

## Bonus

### Adding Go app:

```bash
$ kubectl get po,sts,svc,pvc
NAME                                        READY   STATUS    RESTARTS       AGE
pod/app-go-chart-0                          2/2     Running   0              35s
pod/app-go-chart-1                          2/2     Running   0              35s
pod/app-python-chart-0                      2/2     Running   0              70m
pod/app-python-chart-1                      2/2     Running   0              70m
pod/vault-0                                 1/1     Running   2 (134m ago)   13d
pod/vault-agent-injector-5cd8b87c6c-fhxvs   1/1     Running   2 (134m ago)   13d

NAME                                READY   AGE
statefulset.apps/app-go-chart       2/2     35s
statefulset.apps/app-python-chart   2/2     70m
statefulset.apps/vault              1/1     13d

NAME                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
service/app-go-chart               NodePort    10.109.23.56    <none>        80:32747/TCP        35s
service/app-python-chart           NodePort    10.107.129.32   <none>        80:31137/TCP        70m
service/kubernetes                 ClusterIP   10.96.0.1       <none>        443/TCP             28d
service/vault                      ClusterIP   10.107.161.46   <none>        8200/TCP,8201/TCP   13d
service/vault-agent-injector-svc   ClusterIP   10.109.31.149   <none>        443/TCP             13d
service/vault-internal             ClusterIP   None            <none>        8200/TCP,8201/TCP   13d

NAME                                                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/appdata-go-app-go-chart-0       Bound    pvc-67e77e5b-4991-4c6d-9dcc-bf49166e3725   15Mi       RWO            standard       35s
persistentvolumeclaim/appdata-go-app-go-chart-1       Bound    pvc-e3f76113-807b-4ae5-8fd6-78c1fb61521b   15Mi       RWO            standard       35s
persistentvolumeclaim/appdata-py-app-python-chart-0   Bound    pvc-d1b41986-e827-415e-b20b-90a19bb8cb4c   15Mi       RWO            standard       70m
persistentvolumeclaim/appdata-py-app-python-chart-1   Bound    pvc-41f4c94b-7e79-4dcd-9c40-4334e987c2ab   15Mi       RWO            standard       70m
```

```bash
$ kubectl exec pod/app-go-chart-0 -- cat /appdata/visits.txt
221
```

```bash
$ kubectl exec pod/app-go-chart-1 -- cat /appdata/visits.txt
208
```


### Update Strategies

#### Rolling Update
Rolling Update strategy allows you to update your application with zero downtime. It does this by gradually replacing old Pods with new ones. The strategy ensures that at least a certain number of Pods are available at all times during the update process.

#### Recreate Update
The Recreate strategy is a simpler strategy where all existing Pods are terminated before new ones are created. This strategy results in a certain period of downtime as all Pods are unavailable during the update process.

In most cases, Rolling strategy is recommended. However, some difficult cases (for example, if we are running a stateful application where pods communicate with each other) might require using Recreate.