# Task 1

```bash
ublunta@lenovo:~/Projects/core-course-labs/k8s$ helm install --dry-run --debug python ./app-python/
install.go:214: [debug] Original chart version: ""
install.go:231: [debug] CHART PATH: /home/ublunta/Projects/core-course-labs/k8s/app-python

NAME: python
LAST DEPLOYED: Wed Nov 29 01:05:24 2023
NAMESPACE: default
STATUS: pending-install
REVISION: 1
USER-SUPPLIED VALUES:
{}

COMPUTED VALUES:
affinity: {}
autoscaling:
  enabled: false
  maxReplicas: 100
  minReplicas: 1
  targetCPUUtilizationPercentage: 80
fullnameOverride: ""
image:
  pullPolicy: Always
  repository: seakysneka1/webserv_python
  tag: latest
imagePullSecrets: []
ingress:
  annotations: {}
  className: ""
  enabled: true
  hosts:
  - host: app-python
    paths:
    - path: /
      pathType: ImplementationSpecific
  tls: []
library-chart:
  global: {}
nameOverride: ""
nodeSelector: {}
podAnnotations:
  vault.hashicorp.com/agent-inject: "true"
  vault.hashicorp.com/agent-inject-secret-database-config.txt: internal/data/database/config
  vault.hashicorp.com/agent-inject-status: update
  vault.hashicorp.com/agent-inject-template-database-config.txt: |
    {{- with secret "internal/data/database/config" -}}
    {{ .Data.data.username }}:{{ .Data.data.password }}
    {{- end -}}
  vault.hashicorp.com/role: internal-app
podLabels: {}
podManagementPolicy: Parallel
podSecurityContext: {}
replicaCount: 3
resources: {}
securityContext: {}
service:
  port: 8000
  type: LoadBalancer
serviceAccount:
  annotations: {}
  automount: true
  create: true
  name: ""
tolerations: []
volumeClaimTemplates:
- metadata:
    name: counter-data-py
  spec:
    accessModes:
    - ReadWriteOnce
    resources:
      requests:
        storage: 10Mi
volumeMounts:
- mountPath: /config.json
  name: config-volume-py
  subPath: config.json
- mountPath: /data
  name: counter-data-py
volumes:
- configMap:
    name: config-map-entity-py
  name: config-volume-py
- emptyDir:
    sizeLimit: 500Mi
  name: counter-data-py

HOOKS:
---
# Source: app-python/templates/tests/test-connection.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "python-app-python-test-connection"
  labels:
    helm.sh/chart: app-python-0.1.0
    app.kubernetes.io/name: app-python
    app.kubernetes.io/instance: python
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  annotations:
    "helm.sh/hook": test
spec:
  containers:
    - name: wget
      image: busybox
      command: ['wget']
      args: ['python-app-python:8000']
  restartPolicy: Never
---
# Source: app-python/templates/post-install-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "python-post-install"
  annotations:
    "helm.sh/hook": post-install
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  template:
    metadata:
      name: "python-post-install"
      labels:
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/instance: "python"
        helm.sh/chart: "app-python-0.1.0"
    spec:
      restartPolicy: Never
      containers:
      - name: post-install-job
        image: busybox
        command: ['sh', '-c', 'echo The post-install hook is running && sleep 10' ]
---
# Source: app-python/templates/pre-install-hook.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: "python-pre-install"
  annotations:
    "helm.sh/hook": pre-install
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  template:
    metadata:
      name: "python-pre-install"
      labels:
        app.kubernetes.io/managed-by: "Helm"
        app.kubernetes.io/instance: "python"
        helm.sh/chart: "app-python-0.1.0"
    spec:
      restartPolicy: Never
      containers:
      - name: pre-install-job
        image: busybox
        command: ['sh', '-c', 'echo The pre-install hook is running && sleep 10' ]
MANIFEST:
---
# Source: app-python/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: python-app-python
  labels:
    helm.sh/chart: app-python-0.1.0
    app.kubernetes.io/name: app-python
    app.kubernetes.io/instance: python
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: app-python/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: secret-pass
type: Opaque
data:
  pass_data: cXdlcnR5MTIz
---
# Source: app-python/templates/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: config-map-entity-py
  namespace: default
data:
  test: test
  config.json: |-
    {
        "key_important1": "value_important1",
        "key_important2": "value_important2"
    }
---
# Source: app-python/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: python-app-python
  labels:
    helm.sh/chart: app-python-0.1.0
    app.kubernetes.io/name: app-python
    app.kubernetes.io/instance: python
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: LoadBalancer
  ports:
    - port: 8000
      targetPort: http
      protocol: TCP
      name: http
  selector:
    app.kubernetes.io/name: app-python
    app.kubernetes.io/instance: python
---
# Source: app-python/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: python-app-python
  labels:
    helm.sh/chart: app-python-0.1.0
    app.kubernetes.io/name: app-python
    app.kubernetes.io/instance: python
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 3
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app.kubernetes.io/name: app-python
      app.kubernetes.io/instance: python
  template:
    metadata:
      annotations:
        vault.hashicorp.com/agent-inject: "true"
        vault.hashicorp.com/agent-inject-secret-database-config.txt: internal/data/database/config
        vault.hashicorp.com/agent-inject-status: update
        vault.hashicorp.com/agent-inject-template-database-config.txt: |
          {{- with secret "internal/data/database/config" -}}
          {{ .Data.data.username }}:{{ .Data.data.password }}
          {{- end -}}
        vault.hashicorp.com/role: internal-app
      labels:
        helm.sh/chart: app-python-0.1.0
        app.kubernetes.io/name: app-python
        app.kubernetes.io/instance: python
        app.kubernetes.io/version: "1.16.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: internal-app
      securityContext:
        {}
      containers:
        - name: app-python
          securityContext:
            {}
          image: "seakysneka1/webserv_python:latest"
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 8000
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /
              port: http
          readinessProbe:
            httpGet:
              path: /
              port: http
          resources:
            {}
          volumeMounts:
            - mountPath: /config.json
              name: config-volume-py
              subPath: config.json
            - mountPath: /data
              name: counter-data-py
          env:
            - name: "VAR1"
              value: "value1"
            - name: "VAR2"
              value: "value2"
            - name: "SECRET-PASS"
              valueFrom:
                secretKeyRef:
                  name: secret-pass
                  key: pass_data
            - name: SPECIAL_KEY_PY
              valueFrom:
                configMapKeyRef:
                  name: config-map-entity-py
                  key: test
      volumes:
        - configMap:
            name: config-map-entity-py
          name: config-volume-py
        - emptyDir:
            sizeLimit: 500Mi
          name: counter-data-py
  volumeClaimTemplates:
  - metadata:
      name: counter-data-py
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 10Mi
---
# Source: app-python/templates/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: python-app-python
  labels:
    helm.sh/chart: app-python-0.1.0
    app.kubernetes.io/name: app-python
    app.kubernetes.io/instance: python
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  rules:
    - host: "app-python"
      http:
        paths:
          - path: /
            pathType: ImplementationSpecific
            backend:
              service:
                name: python-app-python
                port:
                  number: 8000

NOTES:
1. Get the application URL by running these commands:
  http://app-python/
```

# Task 2

## outputs of kubectl get
```bash

ublunta@lenovo:~/Projects/core-course-labs/k8s$ kubectl get po,sts,svc,pvc
NAME                      READY   STATUS    RESTARTS   AGE
pod/python-app-python-0   1/1     Running   0          2m29s
pod/python-app-python-1   1/1     Running   0          2m29s
pod/python-app-python-2   1/1     Running   0          2m29s
pod/vault-0               1/1     Running   0          14m

NAME                                 READY   AGE
statefulset.apps/python-app-python   3/3     2m29s
statefulset.apps/vault               1/1     15d

NAME                        TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
service/kubernetes          ClusterIP      10.96.0.1       <none>        443/TCP          14m
service/python-app-python   LoadBalancer   10.99.129.178   <pending>     8000:31777/TCP   2m29s

NAME                                                        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/counter-data-py-python-app-python-0   Bound    pvc-61dbaeb2-7b4b-485a-9807-efbc2be588a2   10Mi       RWO            standard       2m29s
persistentvolumeclaim/counter-data-py-python-app-python-1   Bound    pvc-f280e565-8107-428f-bae1-4b4940d03113   10Mi       RWO            standard       2m29s
persistentvolumeclaim/counter-data-py-python-app-python-2   Bound    pvc-94faed66-aa7a-49f5-af34-4eea2ed95d26   10Mi       RWO            standard       2m29s
```

## 1. Data in volumes
```bash
ublunta@lenovo:~/Projects/core-course-labs/k8s$ kubectl exec python-app-python-0  -- cat /data/counter.json
{"count": 40}
ublunta@lenovo:~/Projects/core-course-labs/k8s$ kubectl exec python-app-python-1  -- cat /data/counter.json
{"count": 43}
ublunta@lenovo:~/Projects/core-course-labs/k8s$ kubectl exec python-app-python-2  -- cat /data/counter.json
{"count": 37}
```

Each pod within a StatefulSet is assigned a unique and stable hostname based on the ordinal index of the pod. For example, if you have a StatefulSet named "web" and you scale it to three replicas, the pads would be named "web-0", "web-1", and "web-2". The ordinal index is significant because it provides a stable identity for each pod, making it easier to manage stateful applications. Every pod has its own storage. That's why they differ

## 2. Ordering Guarantee
In my case, pods of the same type do not have interdependencies. However, ordering may be required if there is a separation, such as one pod being responsible for reading and another for writing.

## 2. Parallel Operations
The accomplishment of this task was achieved through the utilization of the `podManagementPolicy`..

## Bonus: Update Strategies
- **Non-Rolling Update** involves terminating all current instances of an application at once and launching new instances with updated configurations or code. This method may result in downtime, as there is a period when no instances of the application are running.

- **Rolling Update** is a strategy where instances of the application are updated one at a time, gradually replacing old instances with new ones. This approach ensures continuous availability by always having a running version of the application. Kubernetes accomplishes this by incrementally increasing the number of instances with the new version while decreasing the number of instances with the old version.

- **Canary Deployment** is a technique that introduces an update to a small subset of users or instances before deploying it to the entire user base. This allows for practical testing of the update's impact before a broader rollout. If the initial release to this small group is successful, the update is then progressively extended to the remaining instances.