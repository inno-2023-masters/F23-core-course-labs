## Task 1. Kube Prometheus Stack 


### 1. Components


-   *The Prometheus Operator* - performs management of the prometheus deployment on Kubernetes. 
including 
-   *Prometheus* - defines prometheus deployment
-   *Alertmanager* - defines alertmanager deployment. It processes prometheus alerts by grouping, deduplicating and forwarding them to some integrated consumer application, such as Slack.
-   *Prometheus node-exporter* - defines node-exporter deployment which performs gathering 
of the host hardware and OS metrics with use of dedicated collectors
-   *Prometheus adapter* - Prometheus Adapter for Kubernetes Metrics APIs which replaces metrics
server on k8s
-   *kube-state-metrics* - service which gathers metrics of the Kubernetes objects, such as pods,
deployments and services via Kubernetes APIs
-   *Grafana* - performs aggregation and visualization of metrics gathered by prometheus


### 2. Helm chart installation






1. Generate self signed certificates
```
./easyrsa init-pki
./easyrsa --batch build-ca nopass
./easyrsa --subject-alt-name="IP:172.19.0.2,DNS:*.k8s.local" --days=9000 build-server-full  server nopass
cp pki/issued/server.crt ~/repos/kubeadm-prom/
cp pki/private/server.key  ~/repos/kubeadm-prom/
```

2. Create secret for k8s from previously defined keypair
```
secret_name="tls-credential"

kubectl create secret tls $secret_name \
  --key=server.key \
  --cert=server.crt \
  -n default
```



3. Create kind config with ingress controller enabled
```yaml
# cluster.yml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
    protocol: TCP
  - containerPort: 443
    hostPort: 443
    protocol: TCP

```

4. Create kind cluster

````
kind create cluster --config=cluster.yml
````

5. Apply nginx controller manifest
```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml

/static/provider/kind/deploy.yaml
namespace/ingress-nginx created
serviceaccount/ingress-nginx created
serviceaccount/ingress-nginx-admission created
role.rbac.authorization.k8s.io/ingress-nginx created
role.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrole.rbac.authorization.k8s.io/ingress-nginx created
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created
rolebinding.rbac.authorization.k8s.io/ingress-nginx created
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
configmap/ingress-nginx-controller created
service/ingress-nginx-controller created
service/ingress-nginx-controller-admission created
deployment.apps/ingress-nginx-controller created
job.batch/ingress-nginx-admission-create created
job.batch/ingress-nginx-admission-patch created
ingressclass.networking.k8s.io/nginx created
networkpolicy.networking.k8s.io/ingress-nginx-admission created
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created
```

6. Wait until nginx becomes ready
```
kubectl wait --namespace ingress-nginx   --for=condition=ready pod   --selector=app.kubernetes.io/component=controller   --timeout=150s
```

7. Install `kube-prom-stack` helm chart
    


```bash
# Get basic values overrides file which provide services configuration with nginx ingress
wget -o values.yml https://raw.githubusercontent.com/fabianlee/k3s-cluster-kvm/main/roles/k3s-kube-prometheus-stack/files/prom-sparse.expanded.yaml
sed -i "s/kubeadm.local/k8s.local/g" values.yml
sed -i "s/'cluster': 'k3s'/'cluster': 'kind-kind'/" values.yml


# TODO: add
podMonitorSelectorNilUsesHelmValues: false
serviceMonitorSelectorNilUsesHelmValues: false
# to `prometheus.prometheusSpec` in values.yml


helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

helm install   --namespace prom   -f valus.yml   prom-stack prometheus-community/kube-prometheus-stack
```

```
kubectl -n prom get po,sts,svc,pvc,cm

NAME                                                      READY   STATUS    RESTARTS   AGE
pod/prom-stack-grafana-7769f8cccb-xlz9s                   3/3     Running   0          31m
pod/prom-stack-kube-prometheus-operator-ddf99c4db-mn79m   1/1     Running   0          31m
pod/prom-stack-kube-state-metrics-66fb8df95c-p7mmb        1/1     Running   0          31m
pod/prom-stack-prometheus-node-exporter-ch2pt             1/1     Running   0          31m
pod/prometheus-prom-stack-kube-prometheus-prometheus-0    2/2     Running   0          31m

NAME                                                                READY   AGE
statefulset.apps/prometheus-prom-stack-kube-prometheus-prometheus   1/1     31m

NAME                                              TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
service/prom-stack-grafana                        ClusterIP   10.96.47.38     <none>        80/TCP              31m
service/prom-stack-kube-prometheus-alertmanager   ClusterIP   10.96.5.95      <none>        9093/TCP,8080/TCP   31m
service/prom-stack-kube-prometheus-operator       ClusterIP   10.96.50.239    <none>        443/TCP             31m
service/prom-stack-kube-prometheus-prometheus     ClusterIP   10.96.135.220   <none>        9090/TCP,8080/TCP   31m
service/prom-stack-kube-state-metrics             ClusterIP   10.96.204.217   <none>        8080/TCP            31m
service/prom-stack-prometheus-node-exporter       ClusterIP   10.96.124.63    <none>        9100/TCP            31m
service/prometheus-operated                       ClusterIP   None            <none>        9090/TCP            31m

NAME                                                                     DATA   AGE
configmap/kube-root-ca.crt                                               1      42m
configmap/prom-stack-grafana                                             1      31m
configmap/prom-stack-grafana-config-dashboards                           1      31m
configmap/prom-stack-kube-prometheus-alertmanager-overview               1      31m
configmap/prom-stack-kube-prometheus-apiserver                           1      31m
configmap/prom-stack-kube-prometheus-cluster-total                       1      31m
configmap/prom-stack-kube-prometheus-controller-manager                  1      31m
configmap/prom-stack-kube-prometheus-etcd                                1      31m
configmap/prom-stack-kube-prometheus-grafana-datasource                  1      31m
configmap/prom-stack-kube-prometheus-grafana-overview                    1      31m
configmap/prom-stack-kube-prometheus-k8s-coredns                         1      31m
configmap/prom-stack-kube-prometheus-k8s-resources-cluster               1      31m
configmap/prom-stack-kube-prometheus-k8s-resources-multicluster          1      31m
configmap/prom-stack-kube-prometheus-k8s-resources-namespace             1      31m
configmap/prom-stack-kube-prometheus-k8s-resources-node                  1      31m
configmap/prom-stack-kube-prometheus-k8s-resources-pod                   1      31m
configmap/prom-stack-kube-prometheus-k8s-resources-workload              1      31m
configmap/prom-stack-kube-prometheus-k8s-resources-workloads-namespace   1      31m
configmap/prom-stack-kube-prometheus-kubelet                             1      31m
configmap/prom-stack-kube-prometheus-namespace-by-pod                    1      31m
configmap/prom-stack-kube-prometheus-namespace-by-workload               1      31m
configmap/prom-stack-kube-prometheus-node-cluster-rsrc-use               1      31m
configmap/prom-stack-kube-prometheus-node-rsrc-use                       1      31m
configmap/prom-stack-kube-prometheus-nodes                               1      31m
configmap/prom-stack-kube-prometheus-nodes-darwin                        1      31m
configmap/prom-stack-kube-prometheus-persistentvolumesusage              1      31m
configmap/prom-stack-kube-prometheus-pod-total                           1      31m
configmap/prom-stack-kube-prometheus-prometheus                          1      31m
configmap/prom-stack-kube-prometheus-proxy                               1      31m
configmap/prom-stack-kube-prometheus-scheduler                           1      31m
configmap/prom-stack-kube-prometheus-workload-total                      1      31m
configmap/prometheus-prom-stack-kube-prometheus-prometheus-rulefiles-0   34     31m

```
- Get kind master node IP 
```bash
MASTER_IP=$(kubectl get node kind-control-plane -o wide  | awk -v OFS='\t\t' '{print $6}'  | tail -n 1)

sudo sh -c "echo -e '
${MASTER_IP} prometheus.k8s.local
${MASTER_IP} grafana.k8s.local
' >> /etc/hosts"

```



### 3. Observations from Grafana dashboard



- Default credentials for Grafana are `admin:prom-operator`


    - Access Grafana using `minikube service monitoring-grafana`.
    - Explore existing dashboards to find information about your cluster:
        1. Check CPU and Memory consumption of your StatefulSet.
	        - CPU consumption ![](/assets/screenshots/k8s-grafana-cpu-consumption.png)

	        -  Memory consumption ![](/assets/screenshots/k8s-grafana-memory-consumption.png)

	        - ![](/assets/screenshots/k8s-grafana-memory-consumption-2.png)

        1. Identify Pods with higher and lower CPU usage in the default namespace.
	        - Since prometheus stack was deployed in dedicated namespace, it's workloads are not showing in the default namespace. Hence, pods with highest and lowest CPU usage are  related to `app-go` workload
	        - ![](/assets/screenshots/k8s-grafana-cpu-usage-by-pod.png)

        1. Monitor node memory usage in percentage and megabytes.
	        -  ![](/assets/screenshots/k8s-grafana-memory-usage-node.png)

        3. Count the number of pods and containers managed by the Kubelet service.
	        -   ![](/assets/screenshots/k8s-grafana-kubelet.png)

        4. Evaluate network usage of Pods in the default namespace.
	        - ![](/assets/screenshots/k8s-grafana-network-usage.png)

        1. Determine the number of active alerts; also check the Web UI with `minikube service monitoring-kube-prometheus-alertmanager`. 
	        - ![](/assets/screenshots/k8s-grafana-alertmanager.png)

	        - Currently there is no active alerts
    
	

## Task 2. Init containers

- The following init container was created in order to fetch `config.json` file using `wget` 
```
initContainers:
  - name: config-downloader
    image: busybox:1.28
    command: ['sh', '-c', 'sleep 5 && wget -O /app/init-data/config.json https://raw.githubusercontent.com/lcensies/core-course-labs/lab14/k8s/app-go/helm/files/config.json' ]
    volumeMounts:
      - name: init-data
        mountPath: /app/init-data
```

- Verify containers status during initialization:
    - `Init:0/1` indicates that currently init container is running
```
watch -n 1 kubectl get po

NAME       READY   STATUS     RESTARTS   AGE
app-go-0   1/1     Running    0          8s
app-go-1   0/1     Init:0/1   0          1s
```


- Verify that the file is present in volume mounted to `app-go`   using ephemeral container:
```
cat /proc/$APP_PID/root/app/init-data/config.json
{
    "why": 42
}
```


## Bonus

### Exporting application metrics to prometheus

- In order to link metrics endpoint of the application to the Prometheus, ServiceMonitor CustomResourceDefinition is needed
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app: app-go
    serviceMonitorSelector: prometheus
  name: app-go-metrics-monitor
spec:
  endpoints:
    - interval: 30s
    - path: /metrics
  selector:
    matchLabels:
      app.kubernetes.io/name: app-go

```




- We can verify that the service is working by checking list of prometheus targets

![](/assets/screenshots/k8s-prom.png)




### Init containers queue

- The following queue of init containers was implemented. In order to ensure that order is preserved, larger sleep delay was
added to the first contianer

```yaml
initContainers:
  - name: queued-writer-1
    image: busybox:1.28
    command: ['sh', '-c', 'sleep 1.5 && echo "Hello from first init container in queue" > /app/init-data/init_ordered_access_demo.txt' ]
    volumeMounts:
      - name: init-data
        mountPath: /app/init-data
  - name: queued-writer-2
    image: busybox:1.28
    command: ['sh', '-c', 'sleep 1 && echo "Hello from second init container in queue" >> /app/init-data/init_ordered_access_demo.txt' ]
    volumeMounts:
      - name: init-data
        mountPath: /app/init-data
  - name: queued-writer-3
    image: busybox:1.28
    command: ['sh', '-c', 'sleep 1 && echo "Hello from third init container in queue" >> /app/init-data/init_ordered_access_demo.txt' ]
    volumeMounts:
      - name: init-data
        mountPath: /app/init-data

```

- We can verify that writes to file were sequential with use of ephemeral container

```
/proc/1/root/app # cat /proc/1/root/app/init-data/init_ordered_access_demo.txt 
Hello from first init container in queue
Hello from second init container in queue
Hello from third init container in queue
```


## References

https://github.com/prometheus-operator/kube-prometheus#kubeprometheus

https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/getting-started.md - shows the example with ServiceMonitor

https://github.com/prometheus-operator/kube-prometheus/blob/main/docs/kube-prometheus-on-kubeadm.md - simple setup, but without helm

https://fabianlee.org/2022/07/02/prometheus-exposing-prometheus-grafana-as-ingress-for-kube-prometheus-stack/ - k3s + helm 

https://fabianlee.org/2022/05/25/kvm-kubeadm-cluster-on-kvm-using-ansible/ -  kubeadm + helm
