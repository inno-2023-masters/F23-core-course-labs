version: '3.7'

x-logging: &logging
  driver: "json-file"
  options:
    max-size: "200m"
    max-file: "10"
    tag: "{{.ImageName}}|{{.Name}}"


x-healthcheck: &healthcheck
  interval: 5s
  timeout: 10s
  retries: 20

services:
  app_python:
    image: madfisher/server:latest
    command: python -m poetry run uvicorn server.app:app --host 0.0.0.0
    mem_limit: 128m
    env_file:
      - ../app_python/.env
    volumes:
      - ../app_python:/app
    ports:
      - "8000:8000"
    logging: *logging
    healthcheck:
      <<: *healthcheck
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]

  app_go:
    image: madfisher/app_go-server:latest
    command: /app
    mem_limit: 256m
    volumes:
      - ../app_go:/app
    ports:
      - "8010:8000"
    logging: *logging
    healthcheck:
      <<: *healthcheck
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]

  grafana:
    image: grafana/grafana
    container_name: monitoring.grafana
    volumes:
      - grafana-data:/var/lib/grafana
      - ${PWD}/grafana/provisioning:/etc/grafana/provisioning
    environment:
      - GF_SECURITY_ADMIN_USER=${ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}
    restart: unless-stopped
    expose:
      - 3000
    ports:
      - "3000:3000"
    logging: *logging
    healthcheck:
      <<: *healthcheck
      test: [ "CMD", "curl", "-f", "http://localhost:3000/api/health" ]

  promtail:
    image: grafana/promtail
    container_name: monitoring.promtail
    mem_limit: 64m
    expose:
      - 9080
    ports:
      - "9080:9080"
    volumes:
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - promtail-data:/var/lib/promtail/positions
      - ${PWD}/promtail/promtail.yml:/etc/promtail/promtail.yml
    command: -config.file=/etc/promtail/promtail.yml
    restart: unless-stopped
    logging: *logging
    healthcheck:
      <<: *healthcheck
      test: [ "CMD", "curl", "-f", "http://localhost:9080/health" ]

  prometheus:
    image: prom/prometheus
    container_name: monitoring.prometheus
    mem_limit: 64m
    expose:
      - 9090
    ports:
      - "9090:9090"
    volumes:
      - ${PWD}/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=14d'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.wal-compression'
      - '--storage.tsdb.max-block-duration=4h'
      - '--storage.tsdb.wal-segment-size=60MB'
      - '--storage.tsdb.allow-overlapping-blocks'
      - '--query.max-samples=5000000'
    restart: unless-stopped
    logging: *logging
    healthcheck:
      <<: *healthcheck
      test: [ "CMD", "curl", "-f", "http://localhost:9090/-/healthy" ]

  loki:
    image: grafana/loki
    container_name: monitoring.loki
    mem_limit: 64m
    expose:
      - 3100
    ports:
      - "3100:3100"
    volumes:
      - ${PWD}/loki/loki-config.yml:/etc/loki-config.yml
      - loki-data:/tmp/loki/
    command: -config.file=/etc/loki-config.yml
    restart: unless-stopped
    logging: *logging
    healthcheck:
      <<: *healthcheck
      test: [ "CMD", "curl", "-f", "http://localhost:3100/ready" ]

volumes:
  grafana-data: {}
  prometheus-data: {}
  loki-data: {}
  promtail-data: {}